FROM nvidia/cuda:12.6.0-devel-ubuntu24.04

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip python3-venv \
    build-essential cmake git curl \
    && rm -rf /var/lib/apt/lists/*

RUN echo ">>> Building llama.cpp: CUDA GPU <<<"
RUN git clone https://github.com/ggerganov/llama.cpp /llama.cpp --depth=1

# libcuda.so.1 is a driver lib only on the host â€” use the stub for building
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 &&     echo "/usr/local/cuda/lib64/stubs" > /etc/ld.so.conf.d/cuda-stubs.conf &&     ldconfig

RUN cmake -B /llama.cpp/build -S /llama.cpp \
    -DCMAKE_BUILD_TYPE=Release \
    -DLLAMA_CURL=OFF \
    -DGGML_CUDA=ON \
    -DLLAMA_BUILD_TESTS=OFF \
    -DLLAMA_BUILD_EXAMPLES=OFF \
    -DLLAMA_BUILD_BENCH=OFF

RUN cmake --build /llama.cpp/build --config Release -j$(nproc) --target llama-server --target llama-cli ||     cmake --build /llama.cpp/build --config Release -j$(nproc) --target server

RUN find /llama.cpp/build/bin -type f -executable 2>/dev/null || echo "No executables in bin/"
RUN find /llama.cpp/build -name "llama-server" 2>/dev/null || echo "llama-server NOT FOUND"
RUN test -f /llama.cpp/build/bin/llama-server || (echo "ERROR: llama-server was not built!" && exit 1)
RUN echo ">>> llama.cpp CUDA build complete <<<"

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt --break-system-packages

COPY config.py ingest.py rag_api.py download_model.py benchmark.py ./
COPY templates/ templates/

EXPOSE 8000 8080

CMD ["uvicorn", "rag_api:app", "--host", "0.0.0.0", "--port", "8000"]
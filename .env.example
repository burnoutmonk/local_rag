LLM_MODEL_FILE=qwen2.5-3b-instruct-q4_k_m.gguf
LLM_MODEL_REPO=Qwen/Qwen2.5-3B-Instruct-GGUF
LLM_MODEL_PATH=models/qwen2.5-3b-instruct-q4_k_m.gguf

# ── LLM server ────────────────────────────────────────────────────────────────
LLM_CONTEXT=4096
LLM_THREADS=8
# GPU layers: set to -1 to offload all layers to GPU, 0 for CPU only
LLM_GPU_LAYERS=0
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.8
LLM_TOP_K=20
LLM_MIN_P=0.0

# ── Output tokens ─────────────────────────────────────────────────────────────
# slow CPU: 200-300 | fast CPU/GPU: 500-900
MAX_TOKENS=500
MIN_TOKENS=150
# run test_speed.py to measure your hardware
TOKENS_PER_SECOND=10.0

# ── Chunking ──────────────────────────────────────────────────────────────────
MAX_CHARS=1000
OVERLAP_CHARS=100

# ── GPU build (Docker only) ───────────────────────────────────────────────────
# set to true to build llama.cpp with CUDA support
CUDA_AVAILABLE=false

LLM_MODEL_FILE=Llama-3.2-3B-Instruct-Q4_K_M.gguf
LLM_MODEL_REPO=bartowski/Llama-3.2-3B-Instruct-GGUF
LLM_MODEL_PATH=models/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# ── LLM server ────────────────────────────────────────────────────────────────
# CPU: 4096  |  GPU: 32768 (or higher)
LLM_CONTEXT=4096 
LLM_THREADS=8
# GPU layers: set to -1 to offload all layers to GPU, 0 for CPU only
LLM_GPU_LAYERS=0
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.8
LLM_TOP_K=20
LLM_MIN_P=0.0

# ── Output tokens ─────────────────────────────────────────────────────────────
# slow CPU: 200-300 | fast CPU/GPU: 500-900
MAX_TOKENS=500
MIN_TOKENS=150
# run test_speed.py to measure your hardware
TOKENS_PER_SECOND=10.0

# ── Chunking ──────────────────────────────────────────────────────────────────
MAX_CHARS=1000
OVERLAP_CHARS=100

# ── GPU build (Docker only) ───────────────────────────────────────────────────
# set to true to build llama.cpp with CUDA support
CUDA_AVAILABLE=true
